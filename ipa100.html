<html>
<head>
<!-- #BeginEditable "doctitle" --> 
<TITLE>managment social et environnemental</TITLE>
<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=ISO-8859-1">
<META NAME="Author" LANG="fr" CONTENT="Yves Kocher">
<META NAME="Description" CONTENT="Outils de management et de gestion des syst&egrave;mes sociaux et de l'environnement, bas&eacute; sur le concept du d&eacute;veloppement complexe (d&eacute;veloppement durable). ">
<META NAME="Keywords" LANG="fr" CONTENT="managment, d&eacute;veloppement, complexit&eacute;, entropie, auto-organisation, model, identit&eacute;, culture, &eacute;ducation, &eacute;conomie, politique, social, organisation, diff&eacute;renciation, th&eacute;orie, &eacute;volution, comptabilit&eacute;, gestion, ressource, capital, hi&eacute;rarchie, physique, biologique, psychologie, spirituel, dynamiques, chaos, ordre, d&eacute;sordre, croyance, champ, d&eacute;gradation, d&eacute;pendance, d&eacute;complexification, d&eacute;centrage, connexion, r&eacute;seau, consommation, contraintes, connaissance, confiance, coh&eacute;rence, charge, produit, d&eacute;s&eacute;quilibre, d&eacute;sorganisation, d&eacute;diff&eacute;renciation, diffusion, don, durabilit&eacute;, long, court, terme, &eacute;change, &eacute;chelle. fractal, &eacute;l&eacute;ment, &eacute;nergie, environnement, &eacute;quilibre, phase, flux, hi&eacute;rarchisation, individu, information, int&eacute;gration, interaction, liaison, local, global, niveau, production, r&eacute;ciprocit&eacute;, relation, reproduction, rupture, soci&eacute;t&eacute;, sp&eacute;cialisation, spirale, strat&eacute;gie, structure, symbole, syst&egrave;me, sym&eacute;trique, temps, valeur, vie, virtuel, bilan, compte, exploitation, complexe, lien, particules, chimie, biologie ">
<META NAME="Identifier-URL" CONTENT="http://www.microglobal.ch">
<META NAME="Reply-to" CONTENT="yves@bati.ch">
<META NAME="revisit-after" CONTENT="10">
<META NAME="Publisher" CONTENT="Yves Kocher">
<META NAME="Copyright" CONTENT="ï¿½">
<!-- #EndEditable --> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
</head>

<body bgcolor="#CCCCCC">
<table cellspacing="0" cellpadding="0" border="0">
  <tr> 
    <td colspan="2" valign="top"> <p align="justify">&nbsp;</p>
      <p align="center"><strong><font size="5">INFORMATION</font></strong></p>
      <p align="justify">&nbsp;</p>
      <p align="justify"><strong>L'information</strong> est une notion li&egrave;e 
        &agrave; une utilit&eacute;, &agrave; un objectif sp&eacute;cifique et 
        une strat&eacute;gie &eacute;volutive. Que se soit le <a href="http://www.maxwellian.demon.co.uk/name.html" target="_blank">d&eacute;mon 
        de Maxwell</a> qui trie les mol&eacute;cules chaudes et les mol&eacute;cules 
        froides ou que se soit le <a href="http://www.dannen.com/szilard.html" target="_blank">d&eacute;mon 
        de Szilard</a> qui localise une particule en mouvement pour la faire heurter 
        une parroi, ils ont chacun un objectif sp&eacute;cifique, produire de 
        l'&eacute;nergie &agrave; partir du mouvement thermique al&eacute;atoire 
        des particules. L'exemple de Jacques Hebenstreit (universalis Encyclopaedia 
        2002 V.12 p,118) conceranat la recherched'un document dans une pile, nous 
        montre que l''information sur la couleur des documents recherch&eacute; 
        qpermet de retrouver rapidement un document sp&eacute;cifique se trouvant 
        dans une pochette bleue. L'information est donc un concept relatif &agrave; 
        une <strong>notion d'utilit&eacute;</strong> (production d'&eacute;nergie, 
        recherche d'un document) et de fonction de l'information. </p>
      <p align="justify">Plus g&eacute;n&eacute;ralement, les interactions sont 
        une r&eacute;alit&eacute; que les syst&egrave;mes analysent comme &eacute;tant 
        des informations utiles pour la construction de leur propre identit&eacute;, 
        pour &eacute;voluer vers leurs propres objectifs. Les &eacute;v&eacute;nements 
        des niveaux inf&eacute;rieurs (n-x, x&gt;0), ainsi que les &eacute;v&eacute;nement 
        du niveau de r&eacute;f&eacute;rence pour l'analyse (n) constituent du 
        capital lorsqu'ils sont utilis&eacute; en tant qu'nformation pour les 
        syst&egrave;mes. Le <strong>capital informationnel</strong> est la capacit&eacute; 
        &agrave; percevoir &agrave; travers les interactions, des informations, 
        donc &agrave; traiter des donn&eacute;es de l'environnement interne et 
        externe, pour optimiser son propre fonctionnement. L'information peut 
        donc &ecirc;tre comprise comme &eacute;tant le rapport entre des &eacute;v&eacute;nements 
        et des objectifs. C'est la dynamique organisationelle qui, au moyen de 
        l'information, permet de produire sa propre identit&eacute;. La dynamique 
        de chaque syst&egrave;me produit une conscience de type informative sur 
        les relations entre les &eacute;l&eacute;ments qui constitue le syst&egrave;me. 
        Les relations entre les &eacute;l&eacute;ments, leur logique &eacute;volutive 
        sont les informations manipul&eacute;es par le syst&egrave;me pour engendrer 
        avec plus d'efficacit&eacute; son propre d&eacute;veloppement. </p>
      <p align="justify">L'&eacute;mergence syst&eacute;mique est la capacit&eacute; 
        d'un syst&egrave;me &agrave; consid&eacute;rer la logique &eacute;volutive 
        de ses &eacute;l&eacute;ments comme source d'information pour mieux contr&ocirc;ler 
        et manipuler ses &eacute;l&eacute;ments dans le dessin de d&eacute;velopper 
        le syst&egrave;me. Comprendre la logique &eacute;volutive, c'est acqu&eacute;rir 
        des informations et les utiliser &agrave; travers les logiques fondamentales 
        du d&eacute;veloppement complexe. Plus il a d'informations, plus le syst&egrave;me 
        s'organise et optimise son fonctionnement en prennant des options qui 
        sont les moins al&eacute;atoire possibles. L'information r&eacute;duit 
        en effet les choix et donc les sources d'erreur dans les choix &agrave; 
        effectuer. Plus il y a d'information, plus les choix sont rationnels par 
        rapport &agrave; l'objectif du syst&egrave;me et moins il y a d'&eacute;volution 
        al&eacute;atoire. </p>
      <p align="justify">Ainsi, pour un <strong>syst&egrave;me social</strong>, 
        dont l'&eacute;mergence est le symbole, l'information qui permettera d'optimiser 
        la production symbolique, la connaissance, est constitu&eacute; des flux 
        symboliques, &agrave; travers des interactions symboliques. Les flux symboliques 
        sont l'ensemble des informations que le syst&egrave;me donne, re&ccedil;oit, 
        &eacute;change et partage. Cette information constitue le capital social, 
        qu'elle soit de l'information pure (connaissance) ou des objets ayant 
        une valeur symbolique, donc une valeure sociale. L'information des niveaux 
        inf&eacute;rieurs (biologique ou physique) est donc aussi capable de construire 
        le capital social car ils ont une valeure symbolique dans le niveau social. 
        L'inverse n'est &eacute;videmment pas vrai. Le niveau biologique ne peut 
        pas int&eacute;grer la dimention sociale dans sa dynamique &eacute;volutive. 
        Le social constitue pour le biologique une contrainte d'ordre sup&eacute;rieure. 
      </p>
      <p align="justify">Au <strong>niveau biologique</strong>, l'information 
        est stock&eacute;e dans les formes (3D), notamenet l'ADN, qui constitue 
        le capital biologique. L'appartenance ou non d'une mol&eacute;cule &agrave; 
        un organisme, se r&eacute;alise par la reconnaissance des formes &agrave; 
        travers le syst&egrave;me immunitaire. La forme constitue l'information 
        des syst&egrave;mes biologiques alors que les symboles constituent l'information 
        des syst&egrave;mes sociaux. L'interaction entre les membrannes, les prot&eacute;ines 
        et les mol&eacute;cules de l'environnement, produit des informations qui 
        engendrent des cascades de r&eacute;actions &agrave; l'int&eacute;rieur 
        de la cellule. En biologie le concept d'information est semblable aux 
        sciences sociales et aux sciences physiques, comme l'a d&eacute;velopp&eacute; 
        Shannon. Le <em>programme g&eacute;n&eacute;tiqu</em>e est semblable &agrave; 
        l'<em>intelligence</em> des individus. Il est l'actualisation de principes 
        fondamentaux (minimisation des contraintes spatio-temporelles) qui orient 
        la dynamique des syst&egrave;mes. Ces principes sont comparables &agrave; 
        une fonction <strong>f</strong>(x). Les variables sont l'information <strong>x</strong> 
        dans f(<strong>x</strong>) qui alimente ces fonction qui agissent sur 
        les objets (&eacute;l&eacute;ments ou syst&egrave;mes) physiques, biologiques 
        ou sociaux. La signification de l'information prend un sens &agrave; travers 
        le programme g&eacute;n&eacute;tique qui d&eacute;pend des contraintes 
        de l'environnement &agrave; travers les signaux re&ccedil;u de l'environnement, 
        et &agrave; travers la fontion syst&eacute;miques. Les interaction du 
        syst&egrave;me avec son environnement constituent de l'information <strong>i 
        </strong>lorsqu'elle est int&eacute;gr&eacute; dans le calcul de la fonction 
        de d&eacute;veloppement <strong>f(i)</strong>. Rappelons que cette fonction 
        syst&eacute;mique est d&eacute;crite &agrave; travers le concept du champ 
        spiral&eacute;.</p>
      <p align="justify">La m&ecirc;me logique peut &ecirc;tre d&eacute;finie 
        pour les <strong>niveaux physiques</strong>, que se soit les champs (2D), 
        ou l'&eacute;nergie(1D). Les d&eacute;mons de Maxwell et de Szilard travaillent 
        &agrave; ce niveau particulaire &agrave; une dimension. On sait qu'&agrave; 
        ce niveau, on ne peut pas conna&icirc;tre &agrave; la fois la vitese et 
        la position d'une particule. Cela est bien compr&eacute;hensible puisque 
        lorsqu'on se trouve dans un espace &agrave; 1 dimension. Soit cette dimension 
        est le temps, soit elle est l'espace, puisque ces deux grandeurs sont 
        intimenet li&eacute;es. La signification de l'information au niveau particulaire 
        est de deux ordre. Soit elle est relative &agrave; la densit&eacute; dans 
        l'espace, pr&eacute;sidant les ph&eacute;nom&egrave;nes d'entropie, soit 
        elle est relative &agrave; la densit&eacute; dans le temps et elle est 
        relative aux ph&eacute;nom&egrave;nes de complexification. Nous somme 
        ici au coeur du ph&eacute;nom&egrave;ne de minimisation des contraintes 
        spatio-temporelles qui pr&eacute;sident l'&eacute;volution des syst&egrave;mes. 
      </p>
      <p align="justify">Enfin au <strong>niveau virtuel</strong>, de dimension 
        0, les &eacute;l&eacute;ments discrets, des 0 ou des 1 (le bien ou le 
        mal, le nord ou le sud...), se pr&egrave;tent bien &agrave; l'analyse 
        math&eacute;matique qui y est infiniment pr&eacute;cise &agrave; ce niveau, 
        mais seulement &agrave; ce niveau. La connaissance de cet &eacute;tat 
        virtuel peut &ecirc;tre quantifi&eacute;e. Elle ne le sera pas en valeur 
        d'&eacute;nergie qui n'appara&icirc;t qu'au niveau 1 D. Au niveau virtuel, 
        l<strong>'information &eacute;l&eacute;mentaire</strong> est: N/n = 2 
        soit ; 2 &eacute;tats possibles / 1 &eacute;tat d&eacute;finit</p>
      <p align="center"><strong>I = Lg<font size="1">2</font> (N/n) = 1</strong></p>
      <p align="justify">Ainsi, si nous pouvons affirmer qu'un &eacute;l&eacute;ment 
        virtuel est dans l'&eacute;tat 1 ou 0, nous poss&eacute;dons une quantit&eacute; 
        d'information &eacute;l&eacute;mentaire de 1, c'est-&agrave;-dire un capital 
        de 1. L'<strong>entropie</strong> de cet &eacute;l&eacute;ment virtuel 
        est &eacute;gale &agrave; :</p>
      <p align="center"><strong>H = Somme ( (n<font size="1">i</font>/N) lg<font size="1">2</font> 
        N/n<font size="1">i</font>) ) = 1</strong></p>
      <p align="justify">L'entropie est la quantit&eacute; d'information moyenne 
        que va fournir l'exp&eacute;rience de d&eacute;terminer l'&eacute;tat 
        du bit. L'exp&eacute;rience va ordonner parfaitement le syst&egrave;me, 
        puisque le choix d'une variable fixe automatiquement l'autre. En passant 
        au niveau sup&eacute;rieur, celui de l'&eacute;nergie, c'est-&agrave;-dire 
        d'une liaison de type corde qui fige deux &eacute;l&eacute;ments virtuels 
        dans un &eacute;tat d&eacute;finit, on obtient un nouveau type de capital, 
        c'est-&agrave;-dire d'information et d'entropie. Le capital est form&eacute; 
        de la corde qui fige les deux particules virtuelles dans le niveau inf&eacute;rieur 
        et de l'information du niveau sup&eacute;rieur concernant les cordes (&eacute;nergie). 
        Il y a une croissance de la somme (Information + entropie) &agrave; travers 
        l'&eacute;mergence des niveaux. An sein d'un niveau, il y a une transformation 
        entre entropie et information. Il y a donc conservation de la somme I 
        + H = cte, au sein d'un niveau. Avec l'apparition d'un nouveau niveau, 
        d'une nouvelle dimention, il y a apparition d'un nouvel ordre / d&eacute;sordre. 
        L'entropie diminue sur le niveau inf&eacute;rieur, alors qu'elle appara&icirc;t 
        au niveau sup&eacute;rieur. </p>
      <p align="center"><img src="fig077_fichiers/image001.gif" width="544" height="271"></p>
      <p align="justify">Le passage sur le niveau 0 D d'une entropie &eacute;lev&eacute;e 
        &agrave; une entropie basse viole apparement le deuxi&egrave;me principe 
        de la thermodynamique. L'entropie du niveau o D diminue, alors que l'information 
        sur ce m&ecirc;me niveau augmente. En r&eacute;alit&eacute;, puisque l'entropie 
        s'additionne (variable extensive), il est possible de lui additionner 
        la valeur qui appara&icirc;t sur le niveau sup&eacute;rieur (1 D ). Le 
        nouveau niveau exprime un nouveau degr&eacute; de libert&eacute; dans 
        une nouvelle dimension. Il existe un <strong>principe de conservation</strong> 
        <strong>de la somme I + H au sein d'un niveau (horizontal) et de la croissance, 
        non plus strictement de l'entropie, mais de la somme I + H</strong> <strong>entre 
        les niveaux (verticalement)</strong>. Nous observons ais&eacute;ment que 
        lorsdque l'entropie augmente dans un syst&egrave;me, il y a une perte 
        d'information par consommation du capital. </p>
      <p align="justify">Ainsi, la complexification &agrave; travers les niveaux 
        permet une augmentation globale de la somme I + H. Cela permet de justifier 
        la diversification et la complexification des niveaux. Parall&egrave;lement 
        &agrave; cette augmentation, li&eacute;e &agrave; l'augmentation des degr&eacute;s 
        de libert&eacute;, il y a une augmentation du capital. qui fige les &eacute;tats 
        et diminue les degr&eacute;s de libert&eacute;s dans des &eacute;tats 
        &eacute;loign&eacute;s de leurs positions d'&eacute;quilibre, dans laquelle 
        l'entropie est minimum. Nous d&eacute;veloppons cette approche de mani&egrave;re 
        plus compl&egrave;te dans une &eacute;tude sur la structure syst&eacute;miques 
        des particules &eacute;l&eacute;mentaires. </p>
      <p align="justify">L'information existe &agrave; tout les niveaux syst&eacute;miques. 
        Elle correspond &agrave; une composant des capitaux. Transmettre de l'information, 
        c'est transmettre du capital. Donner des biens, des objets qui ont une 
        valeure sociale, c'est donner des symboles et des informations (flux &eacute;ducatifs). 
        Prendre des biens, du savoir, des mati&egrave;res premi&egrave;res, c'est 
        prendre de l'information (flux hi&eacute;rarchiques). Echanger des valeurs 
        symboliques, c'est &eacute;changer de l'information (flux &eacute;conomiques). 
        Partager des symboles, c'est partager de l'information (flux sociaux). 
      </p>
      <p align="justify"> L'information est mesur&eacute;s &agrave; travers sa 
        capacit&eacute; &agrave; diminuer le nombre de possibilit&eacute;, le 
        nombre de variantes ou d'options, lorsque l'on cherche &agrave; atteindre 
        un objectif d&eacute;finit. L'information nous permet de faire le bon 
        choix plus rapidement. L'information utile pour l'organisation, correspond 
        &agrave; des flux en provenance de l'environnement (&eacute;metteur) qui 
        entrent dans le syst&egrave;me (receveur), qu'ils soient de type R, D, 
        H ou L. Le syst&egrave;me a &eacute;galement une strat&eacute;gie d'&eacute;mission 
        d'information dans l'environnement pour produire un environnement plus 
        favorable &agrave; son propre d&eacute;veloppement. L'information &agrave; 
        un objectif d'organisation interne, alors que les autres capitaux constitu&eacute;s 
        d'&eacute;l&eacute;ments de niveaux inf&eacute;rieurs ont pour objectifs 
        la production de l'identit&eacute; du syst&egrave;me (production donn&eacute;e, 
        vendue, prise ou partag&eacute;e). L'information ne correspond pas &agrave; 
        l'ensemble des capitaux. Elle correspond &agrave; la partie des capitaux 
        internes qui produisent de l'organisation. L'information permet de diff&eacute;rencier 
        les &eacute;l&eacute;ments, et ainsi, de faciliter les choix selon des 
        crit&egrave;res internes li&eacute;s aux oblectifs du syst&egrave;me. 
        L'information peut &ecirc;tre consid&eacute;r&eacute;e comme un &eacute;l&eacute;ment 
        au m&ecirc;me titre qu'un consommable qui circule entre les syst&egrave;mes. 
        Le capital mesur&eacute; en terme d'information rejoint le capital mesur&eacute; 
        en terme de capacit&eacute; &agrave; produire des flux, comme nous le 
        d&eacute;crivons dans le concept du d&eacute;veloppement complexe, car 
        <strong>la somme de l'information des niveaux In + I(n-1) + I(n-2) + ... 
        I(n=0) est &eacute;gale &agrave; l'ensemble des capitaux </strong>responsables 
        des flux. </p>
      <p align="justify">On peut distinguer deux type d'&eacute;l&eacute;ments 
        entrant dans les syst&egrave;mes. Ces deux types sont souvent combin&eacute;s. 
        Les outils, les consommables, et les informations. Les premiers ont pour 
        objectifs la <strong>production</strong>, les seconds ont pour objectifs 
        l'<strong>organisation</strong>. Les premiers sont semblables &agrave; 
        des &eacute;l&eacute;ments <strong>statiques</strong> qui sont transform&eacute;s, 
        travaill&eacute;s, model&eacute;s, construits au moyen d'une <strong>dynamique</strong> 
        dont la logique est construite par des logiques tr&egrave;s g&eacute;n&eacute;rales, 
        d&eacute;crites par le concept du d&eacute;veloppement complexe, pouvant 
        &ecirc;tre exprim&eacute;es par des algorithmes. Les variables de ces 
        fonctions sont les informations externes, celles de l'environnement, et 
        internes, celle du syst&egrave;me. L'organisation est rendue possible 
        par les flux hi&eacute;rarchiques qui imposent des contraintes sur l'environnement, 
        donc qui donne des ordres que l'environnement ex&eacute;cute. La cascade 
        des flux hi&eacute;rarchiques au sein d'un syst&egrave;mes permet l'organisation. 
        Observons &eacute;galement que se sont les liens et les ruptures qui conduisent 
        &agrave; la production, &agrave; l'&eacute;laboration de nouveaux produits 
        et de nouveaux services. En revanche, c'est la diff&eacute;renciation 
        qui est &agrave; l'origine de l'information qui produit l'organisation. 
        En effet, l'information permet de diff&eacute;rencier les &eacute;l&eacute;ments, 
        les situations, afin de restreindre les choix pour mieux les orienter. 
      </p>
      <p align="center"><img src="fig078_fichiers/image001.gif" width="506" height="394"> 
      </p>
      <p align="justify">Plus un syst&egrave;me poss&egrave;de du <strong>capital 
        interne</strong>, plus il a les capacit&eacute;s d'utiliser les interactions 
        avec l'environnement pour en faire des informations capable d'optimiser 
        les prises de d&eacute;cisions, donc de diminuer les choix al&eacute;atoires. 
        La diff&eacute;rence fondamentale entre les flux d'informations et les 
        flux d'&eacute;l&eacute;ments identitaires est qu'ils construisent deux 
        capitaux diff&eacute;rents. Le capital identitaire lib&egrave;re progressivement 
        des flux d'&eacute;l&eacute;ments statiques, et le capital informationnel 
        lib&egrave;re progressivement des flux d'information dynamique. Cela est 
        semblable &agrave; des &eacute;l&eacute;ments statiques li&eacute;s entre 
        eux par des interactions dynamiques et donnant ainsi naissance &agrave; 
        de nouveaux &eacute;l&eacute;ments d'ordre sup&eacute;rieur. C'est la 
        raison pour laquelle nous consid&eacute;rons que l'interaction sur un 
        niveau n est de l'information pour ce niveau, alors qu'il sera consid&eacute;r&eacute; 
        comme un &eacute;l&eacute;ment pour le niveau n+1. </p>
      <p align="justify">La diff&eacute;rence entre flux et capitaux est semblable 
        &agrave; la diff&eacute;rence entre le temps et l'espace. Une relation 
        fondamentale qui est &agrave; la base des algorithmes qui g&egrave;rent 
        la construction des syst&egrave;mes, est la dynamique de minimisation 
        des contraintes spatio-temporelles. Dans cette dynamique de minimisation, 
        il exsite une <strong>fonction &eacute;l&eacute;mentaire</strong>, la 
        <strong>minimisation</strong>, des <strong>variables</strong> qui correspondent 
        aux <strong>informations</strong> et des <strong>objets</strong>, les 
        <strong>&eacute;l&eacute;ments, </strong> sur lesquels agit cette fonction. 
        Plus les informations sont compl&ecirc;tes et vastes, plus la minimisation 
        sera efficace et plus le syst&egrave;me &eacute;voluera en interne vers 
        la complexit&eacute;. Si peu d'informations sont disponibles, non pas 
        forc&eacute;ment que le syst&egrave;me n'est pas suffisament en interaction 
        avec son environnement, mais qu'il n'as pas le capital interne suffisant 
        pour transformer les relations en informations. alors le syst&egrave;me 
        minimisera mal ses contraintes, car les boucles de r&eacute;troactions 
        responsable de la dynamique sont peu efficaces.</p>
      <p align="center"><img src="fig079_fichiers/image001.gif" width="505" height="350"></p>
      <p align="justify">Le r&eacute;sultat global des capitaux internes, form&eacute;s 
        &agrave; la fois d'information et d'&eacute;l&eacute;ments, est la production 
        de flux ext&eacute;rieurs. Une pr&eacute;dominance d'&eacute;l&eacute;ments 
        permettera des flux de type gros d&eacute;bits et peu de pression. Une 
        pr&eacute;dominance d'information permettera des faibles d&eacute;bits 
        &agrave; haute pression. Dans le premier cas, la faible organisation associ&eacute; 
        &agrave; la faible performance unitaire, est compens&eacute; par le fort 
        d&eacute;bit. Dans le second cas, le manque d'&eacute;l&eacute;ments est 
        compens&eacute; par une tr&egrave;s bonne organisation. La bonne organisation 
        c'est celle dans laquelle les bons choix sont r&eacute;alis&eacute;s. 
        Le capital est donc form&eacute; d'une composante informationnelle et 
        d'une composante objective (objets des interactions). L'information fournit 
        au syst&egrave;me dynamique les donn&eacute;es pour optimiser les processus.</p>
      <p align="justify">La liaison, ph&eacute;nom&egrave;ne dynamique, fige les 
        &eacute;tats, de sorte qu'elle produit dans le niveau sup&eacute;rieur 
        une nouvelle particule ayant un capital de 4. Sur le niveau sur lequel 
        la dynamique est r&eacute;alis&eacute;e, on parle d'information, alors 
        que sur le niveau sup&eacute;rieur, c'est un &eacute;l&eacute;ment. La 
        capacit&eacute; pour un &eacute;l&eacute;ment &agrave; produire des flux 
        se d&eacute;finit dans les niveaux inf&eacute;rieurs. Par exemple: une 
        machine (n-1), plac&eacute; dans un contexte social (n) pour qu'elle fonctionne, 
        va effectuer un travail d'ordre physique, par exemple usiner une pi&egrave;ce 
        ou d&eacute;placer un objet (n-1). Elle op&egrave;re ainsi sur les niveaux 
        inf&eacute;rieurs. </p>
      <p align="justify">La somme des capitaux d'un syst&egrave;me est &eacute;gal 
        au capital constitu&eacute; par les &eacute;l&eacute;ments des niveaux 
        inf&eacute;rieurs et par le capital constitu&eacute; de l'information 
        r&eacute;gissant le niveau de r&eacute;f&eacute;rence sur lequel l'analyse 
        est men&eacute;e. Le capital permet de produire plus que l'&eacute;l&eacute;ment 
        &agrave; lui m&ecirc;me accumul&eacute;. En effet, en passant d'un niveau 
        &agrave; un autre, on augment les degr&eacute;s de libert&eacute;, donc 
        la capacit&eacute; &agrave; capitaliser. Cependant, si certains capitaux 
        produisent plus de flux que ce qu'ils ont consomm&eacute;es pour &ecirc;tre 
        &eacute;labor&eacute;s (outils), d'autres, les consommables, ne sont que 
        l'accumulation d'un &eacute;l&eacute;ment, le stockage d'un &eacute;l&eacute;ment 
        sous forme &eacute;labor&eacute;, une forme qui n'est pas totalement d&eacute;grad&eacute;e. 
      </p>
      <p align="center"><img src="fig080_fichiers/image001.gif" width="544" height="317"></p>
      <p align="justify">&nbsp;</p>
      <p align="justify">&nbsp;</p>
      <p align="justify">&nbsp;</p>
      </td>
  </tr>
  <tr> 
    <td height="2" colspan="2" valign="top"></td>
  </tr>
</table>
</body>
</html>
